{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=840\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1016\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=960\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=928\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=976\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=944\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=800\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=912\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=992\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=784\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=816\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=904\n",
      "  warnings.warn(\n",
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=824\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    32\n",
      "1    32\n",
      "Name: count, dtype: int64\n",
      "                audio_path  label  \\\n",
      "0    All Non Dys Wav\\1.wav      0   \n",
      "1   All Non Dys Wav\\10.wav      0   \n",
      "2   All Non Dys Wav\\11.wav      0   \n",
      "3   All Non Dys Wav\\12.wav      0   \n",
      "4  All Non Dys Wav\\13,.wav      0   \n",
      "\n",
      "                                       mfcc_features  \\\n",
      "0  [-536.5430908203125, 97.0123519897461, 7.79789...   \n",
      "1  [-435.2642822265625, 80.30461883544922, 3.3133...   \n",
      "2  [-461.0809631347656, 114.24716186523438, 0.501...   \n",
      "3  [-500.19842529296875, 100.60742950439453, -4.4...   \n",
      "4  [-551.3157958984375, 95.65728759765625, -2.064...   \n",
      "\n",
      "                                     chroma_features  \\\n",
      "0  [0.4485311508178711, 0.464842289686203, 0.4395...   \n",
      "1  [0.3792971968650818, 0.39954376220703125, 0.36...   \n",
      "2  [0.37338632345199585, 0.3548054099082947, 0.36...   \n",
      "3  [0.2978111803531647, 0.32145676016807556, 0.37...   \n",
      "4  [0.4390692412853241, 0.4155716598033905, 0.395...   \n",
      "\n",
      "                          spectral_contrast_features  \\\n",
      "0  [22.622229768059945, 10.427688234073024, 12.26...   \n",
      "1  [22.84654863097061, 15.47561092878769, 16.5438...   \n",
      "2  [22.022058231174235, 16.089575087689777, 18.24...   \n",
      "3  [21.847833800869306, 15.647255641707481, 17.11...   \n",
      "4  [22.761941280386772, 12.996983650056585, 14.94...   \n",
      "\n",
      "                                    tonnetz_features  \\\n",
      "0  [0.01275883644891455, 0.023313654552475, 0.053...   \n",
      "1  [-0.004885673408361462, 0.018838056298698804, ...   \n",
      "2  [0.05993043338437089, 0.07364078656445941, 0.0...   \n",
      "3  [-0.004747163748051297, -0.019899536750502832,...   \n",
      "4  [-0.01206625597995071, 0.01348738080479701, 0....   \n",
      "\n",
      "   zero_crossing_rate_features  \n",
      "0                     0.021896  \n",
      "1                     0.024678  \n",
      "2                     0.023492  \n",
      "3                     0.029626  \n",
      "4                     0.033306  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the path to the audio files\n",
    "normal_audio_dir = \"All Non Dys Wav\"\n",
    "disorder_audio_dir = \"All Dys Wav\"\n",
    "\n",
    "# Create a DataFrame to store audio file paths and labels\n",
    "data = []\n",
    "\n",
    "# Load normal audio files\n",
    "for filename in os.listdir(normal_audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(normal_audio_dir, filename)\n",
    "        data.append((audio_path, 0))\n",
    "\n",
    "# Load disorder audio files\n",
    "for filename in os.listdir(disorder_audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(disorder_audio_dir, filename)\n",
    "        data.append((audio_path, 1))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"audio_path\", \"label\"])\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_mfcc_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfccs, axis=1).tolist()\n",
    "\n",
    "# Function to extract Chroma features\n",
    "def extract_chroma_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    return np.mean(chroma, axis=1).tolist()\n",
    "\n",
    "# Function to extract Spectral Contrast features\n",
    "def extract_spectral_contrast_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    return np.mean(spectral_contrast, axis=1).tolist()\n",
    "\n",
    "# Function to extract Tonnetz features\n",
    "def extract_tonnetz_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    return np.mean(tonnetz, axis=1).tolist()\n",
    "\n",
    "# Function to extract Zero-Crossing Rate features\n",
    "def extract_zero_crossing_rate_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    return np.mean(zero_crossing_rate).tolist()\n",
    "\n",
    "# Apply MFCC feature extraction to each audio file\n",
    "df[\"mfcc_features\"] = df[\"audio_path\"].apply(extract_mfcc_features)\n",
    "\n",
    "# Apply Chroma feature extraction to each audio file\n",
    "df[\"chroma_features\"] = df[\"audio_path\"].apply(extract_chroma_features)\n",
    "\n",
    "# Apply Spectral Contrast feature extraction to each audio file\n",
    "df[\"spectral_contrast_features\"] = df[\"audio_path\"].apply(extract_spectral_contrast_features)\n",
    "\n",
    "# Apply Tonnetz feature extraction to each audio file\n",
    "df[\"tonnetz_features\"] = df[\"audio_path\"].apply(extract_tonnetz_features)\n",
    "\n",
    "# Apply Zero-Crossing Rate feature extraction to each audio file\n",
    "df[\"zero_crossing_rate_features\"] = df[\"audio_path\"].apply(extract_zero_crossing_rate_features)\n",
    "\n",
    "# Verify the balance of the dataset\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Check the first few rows to ensure correct labeling\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Shape: (64, 39)\n",
      "Shape of X before reshaping: (64, 39)\n",
      "Shape of X after reshaping: (64, 39, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the features are combined correctly\n",
    "features = []\n",
    "for index, row in df.iterrows():\n",
    "    combined_features = np.concatenate([\n",
    "        row['mfcc_features'],\n",
    "        row['chroma_features'],\n",
    "        row['spectral_contrast_features'],\n",
    "        row['tonnetz_features'],\n",
    "        [row['zero_crossing_rate_features']]\n",
    "    ])\n",
    "    features.append(combined_features)\n",
    "\n",
    "X = np.array(features)\n",
    "\n",
    "# Debug: Print shape of the combined features\n",
    "print(\"Combined Features Shape:\", X.shape)\n",
    "\n",
    "if X.size == 0:\n",
    "    raise ValueError(\"No valid feature arrays found. Check data preprocessing steps.\")\n",
    "\n",
    "# Ensure all combined features have the same length\n",
    "feature_length = X.shape[1]\n",
    "consistent_length = all(len(feature) == feature_length for feature in X)\n",
    "if not consistent_length:\n",
    "    raise ValueError(\"Inconsistent feature lengths found in the dataset.\")\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "# Normalize features using training data mean and std\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "# Debug: Print shape of X before reshaping\n",
    "print(\"Shape of X before reshaping:\", X.shape)\n",
    "\n",
    "# Reshape features to be compatible with Conv1D\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Debug: Print shape of X after reshaping\n",
    "print(\"Shape of X after reshaping:\", X.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save mean and std for use in prediction\n",
    "np.save('mean.npy', mean)\n",
    "np.save('std.npy', std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fasih\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 207ms/step - accuracy: 0.5535 - loss: 0.7077 - val_accuracy: 0.5385 - val_loss: 0.6615\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4700 - loss: 0.6790 - val_accuracy: 0.5385 - val_loss: 0.6504\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6058 - loss: 0.6070 - val_accuracy: 0.6923 - val_loss: 0.5926\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6763 - loss: 0.5624 - val_accuracy: 0.7692 - val_loss: 0.5602\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8199 - loss: 0.5491 - val_accuracy: 0.8462 - val_loss: 0.5323\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8329 - loss: 0.5088 - val_accuracy: 0.7692 - val_loss: 0.4981\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8903 - loss: 0.4488 - val_accuracy: 0.8462 - val_loss: 0.4685\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8591 - loss: 0.4640 - val_accuracy: 0.9231 - val_loss: 0.4331\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9034 - loss: 0.3963 - val_accuracy: 0.9231 - val_loss: 0.4012\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9165 - loss: 0.3456 - val_accuracy: 0.9231 - val_loss: 0.3722\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8642 - loss: 0.3580 - val_accuracy: 0.9231 - val_loss: 0.3461\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9165 - loss: 0.2989 - val_accuracy: 0.9231 - val_loss: 0.3257\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8903 - loss: 0.3204 - val_accuracy: 0.9231 - val_loss: 0.3018\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9400 - loss: 0.2694 - val_accuracy: 1.0000 - val_loss: 0.2759\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8799 - loss: 0.2690 - val_accuracy: 0.8462 - val_loss: 0.2836\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9295 - loss: 0.2400 - val_accuracy: 0.9231 - val_loss: 0.2591\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1643 - val_accuracy: 1.0000 - val_loss: 0.2277\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9869 - loss: 0.1874 - val_accuracy: 1.0000 - val_loss: 0.2132\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9530 - loss: 0.1877 - val_accuracy: 1.0000 - val_loss: 0.2012\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9034 - loss: 0.1910 - val_accuracy: 1.0000 - val_loss: 0.1855\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.1151 - val_accuracy: 1.0000 - val_loss: 0.1828\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9530 - loss: 0.1314 - val_accuracy: 1.0000 - val_loss: 0.1751\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.1259 - val_accuracy: 1.0000 - val_loss: 0.1612\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0861 - val_accuracy: 1.0000 - val_loss: 0.1471\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1056 - val_accuracy: 1.0000 - val_loss: 0.1362\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0491 - val_accuracy: 1.0000 - val_loss: 0.1282\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0674 - val_accuracy: 1.0000 - val_loss: 0.1173\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.0646 - val_accuracy: 1.0000 - val_loss: 0.1088\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0514 - val_accuracy: 1.0000 - val_loss: 0.1011\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0565 - val_accuracy: 1.0000 - val_loss: 0.0989\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.0733 - val_accuracy: 1.0000 - val_loss: 0.1185\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0370 - val_accuracy: 1.0000 - val_loss: 0.1189\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.0548 - val_accuracy: 1.0000 - val_loss: 0.1223\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.0550 - val_accuracy: 1.0000 - val_loss: 0.1130\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0302 - val_accuracy: 1.0000 - val_loss: 0.1150\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0294 - val_accuracy: 1.0000 - val_loss: 0.1200\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0185 - val_accuracy: 0.9231 - val_loss: 0.1253\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 1.0000 - val_loss: 0.1059\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0283 - val_accuracy: 1.0000 - val_loss: 0.0852\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 1.0000 - val_loss: 0.0754\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0180 - val_accuracy: 1.0000 - val_loss: 0.0745\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0753\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 1.0000 - val_loss: 0.0652\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0208 - val_accuracy: 1.0000 - val_loss: 0.0633\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 0.1008\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0236 - val_accuracy: 0.8462 - val_loss: 0.1477\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9765 - loss: 0.0480 - val_accuracy: 1.0000 - val_loss: 0.1055\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 0.0741\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0177 - val_accuracy: 1.0000 - val_loss: 0.0672\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9869 - loss: 0.0334 - val_accuracy: 1.0000 - val_loss: 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Model Implementation\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('dysarthria_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to predict audio class\n",
    "def predict_audio_class(audio_path, model):\n",
    "    # Extract features from the audio file\n",
    "    mfcc_features = extract_mfcc_features(audio_path)\n",
    "    chroma_features = extract_chroma_features(audio_path)\n",
    "    spectral_contrast_features = extract_spectral_contrast_features(audio_path)\n",
    "    tonnetz_features = extract_tonnetz_features(audio_path)\n",
    "    zero_crossing_rate_features = extract_zero_crossing_rate_features(audio_path)\n",
    "    \n",
    "    # Combine the extracted features\n",
    "    combined_features = np.concatenate([mfcc_features, chroma_features, \n",
    "                                        spectral_contrast_features, tonnetz_features,\n",
    "                                        [zero_crossing_rate_features]])\n",
    "    \n",
    "    # Load mean and std from training phase\n",
    "    mean = np.load('mean.npy')\n",
    "    std = np.load('std.npy')\n",
    "    \n",
    "    # Normalize the features using training mean and std\n",
    "    combined_features_normalized = (combined_features - mean) / std\n",
    "    \n",
    "    # Reshape the features to match the input shape of the model\n",
    "    test_input = combined_features_normalized.reshape(1, combined_features_normalized.shape[0], 1)\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(test_input)\n",
    "    \n",
    "    # Print prediction probabilities for debugging\n",
    "    print(\"Prediction probabilities:\", predictions)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    \n",
    "    return \"Dys\" if predicted_class == 1 else \"Non Dys\"\n",
    "\n",
    "# Load the model for prediction\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('dysarthria_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: Female Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Prediction probabilities: [[7.953187e-04 9.992047e-01]]\n",
      "File: MS1.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.01335469 0.9866453 ]]\n",
      "File: MS2.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.36381826 0.6361817 ]]\n",
      "File: MS3.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00200319 0.99799675]]\n",
      "File: MS4.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00932248 0.9906775 ]]\n",
      "File: MS5.wav - Predicted class: Dys\n",
      "Processing folder: Female Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.9922616  0.00773842]]\n",
      "File: ZA1.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Prediction probabilities: [[9.9986148e-01 1.3854507e-04]]\n",
      "File: ZA2.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.99452066 0.00547929]]\n",
      "File: ZA3.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[9.9964607e-01 3.5394626e-04]]\n",
      "File: ZA4.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[9.9952471e-01 4.7535496e-04]]\n",
      "File: ZA5.wav - Predicted class: Non Dys\n",
      "Processing folder: Male Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00221266 0.99778736]]\n",
      "File: AB1.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00945116 0.9905489 ]]\n",
      "File: AB2.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00742184 0.9925782 ]]\n",
      "File: AB3.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00277358 0.9972264 ]]\n",
      "File: AB4.wav - Predicted class: Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[0.00752461 0.99247533]]\n",
      "File: AB5.wav - Predicted class: Dys\n",
      "Processing folder: Male Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Prediction probabilities: [[0.80403435 0.19596565]]\n",
      "File: MA1.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[9.9930394e-01 6.9609634e-04]]\n",
      "File: MA2.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[9.9969828e-01 3.0179086e-04]]\n",
      "File: MA3.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[9.9943620e-01 5.6382874e-04]]\n",
      "File: MA4.wav - Predicted class: Non Dys\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Prediction probabilities: [[0.9960795  0.00392056]]\n",
      "File: MA5.wav - Predicted class: Non Dys\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the testing dataset\n",
    "testing_dataset_dir = \"Testing Dataset\"\n",
    "\n",
    "# Iterate through each subfolder and predict the class of each audio file\n",
    "for subfolder in os.listdir(testing_dataset_dir):\n",
    "    subfolder_path = os.path.join(testing_dataset_dir, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        print(f\"Processing folder: {subfolder}\")\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                audio_path = os.path.join(subfolder_path, filename)\n",
    "                predicted_label = predict_audio_class(audio_path, model)\n",
    "                print(f\"File: {filename} - Predicted class: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Prediction probabilities: [[2.5436416e-04 9.9974567e-01]]\n",
      "Predicted class: Dys\n"
     ]
    }
   ],
   "source": [
    "predicted_label = predict_audio_class(\"32.wav\", model)\n",
    "print(\"Predicted class:\", predicted_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
